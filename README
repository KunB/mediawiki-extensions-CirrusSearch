MediaWiki extension: CirrusSearch
---------------------------------

Installation
------------
Fetch this plugin plugin into your extensions directory.
Make sure you have the curl php library installed (sudo apt-get install php5-curl in Debian.)
You also need to install the Elastica MediaWiki extension.
Add this to LocalSettings.php:
 require_once( "$IP/extensions/Elastica/Elastica.php" );
 require_once( "$IP/extensions/CirrusSearch/CirrusSearch.php" );
 $wgDisableSearchUpdate = true;

Configure your search servers in LocalSettings.php:
 $wgCirrusSearchServers = array( 'elasticsearch0', 'elasticsearch1', 'elasticsearch2', 'elasticsearch3' );
There are other $wgCirrusSearch variables that you might want to change from their defaults.
In production setups, you will very likely want to increase the number of replicas using
$wgCirrusSearchReplicaCount.
If you want to change them then set their new values with $wgCirrusSearchServers in LocalSettings.php.

Now run this script to generate your elasticsearch index:
 php ./maintenance/updateSearchIndexConfig.php

Now remove $wgDisableSearchUpdate = true from LocalSettings.php.  Updates should start heading to elasticsearch.

Next bootstrap the search index by running:
 php ./maintenance/forceSearchIndex.php --forceUpdate --skipLinks --indexOnSkip
 php ./maintenance/forceSearchIndex.php --forceUpdate --skipParse
Note that this can take some time.  For large wikis read Bootstrapping large wikis below.

Once that is complete add this to LocalSettings.php to funnel queries to ElasticSearch:
 $wgSearchType = 'CirrusSearch';


Bootstrapping large wikis
-------------------------
Since most of the load involved in indexing is parsing the pages in php we provide a few options to split the
process into multiple processes.  Don't worry too much about the database during this process.  It can generally
handle more processess indexing then you are likely to be able to spawn.

The --fromId and --toId let you break the job into smaller chunks or restart the process after ctrl-c-ing it.

The --buildChunks parameter lets you either build chunks of a fixed size or a fixed number of chunks.  Since the
process of indexing leaks an unfortunate amount of memory, we suggest using fixed size chunks:
 rm -rf /tmp/index_log
 mkdir /tmp/index_log
 php forceSearchIndex.php --buildChunks 10000 --forceUpdate --skipLinks --indexOnSkip |
   xargs -I{} -t -P4 sh -c 'php {} > /tmp/index_log/$$.log'

forceSearchIndex.php also accepts --queue which can be used to enqueue all of its work onto the job queue rather
than running in process.  They will then be processed by whatever job queue consumers you have set up.  This is
really only a good idea if you have a very robust job queue infrastructure.

The --batch-size parameter controls the number documents read from MySQL and indexed into elasticsearch at
one time.  It defaults to 10 when parsing and 500 when not and those values work pretty well.  Feel free to play
with it but remember that setting it too high while parsing will make each batch take _forever_.  This is really
bad on the job queue because they'll just be killed.  When not parsing having a large number is pretty efficient
because Elasticsearch does a good job parallelizing the link counts and index operations that take up the bulk
of the time when not parsing.  The defaults are generally good for this.

Handling elasticsearch outages
------------------------------
If for some reason in process updates to elasticsearch begin failing you can immediately
set "$wgDisableSearchUpdate = true;" in your LocalSettings.php file to
stop trying to update elasticsearch.  Once you figure out what is wrong with elasticsearch you
should turn those updates back on and then run the following:
php ./maintenance/forceSearchIndex.php --from <whenever the outage started in ISO 8601 format> --deletes
php ./maintenance/forceSearchIndex.php --from <whenever the outage started in ISO 8601 format>

The first command picks up all the deletes that occurred during the outage and
should complete quite quickly.  The second command picks up all the updates
that occurred during the outage and might take significantly longer.


PoolCounter
-----------
CirrusSearch can leverage the PoolCounter extension to limit the number of concurrent searches to
elasticsearch.  You can do this by installing the PoolCounter extension and then configuring it in
LocalSettings.php like so:
 require_once( "$IP/extensions/PoolCounter/PoolCounterClient.php");
 $wgPoolCounterConf = array(
	'CirrusSearch-Search' => array(  // Configuration for all searches
		'class' => 'PoolCounter_Client',
		'timeout' => 30,
		'workers' => 50,
		'maxqueue' => 10,
	)
 );


Upgrading
---------
When you upgrade there three possible cases for maintaining the index:
1.  You must update the index configuration and reindex from source documents.
2.  You must update the index configuration and reindex from already indexed documents.
3.  You must update the index configuration but no reindex is required.
4.  No changes are required.

If you must do (1) you have three options:
A.  Blow away the search index and rebuild it from scratch.  Marginally faster and uses less disk space on
in elasticsearch but empties the index entirely and rebuilds it so search will be down for a while:
 php updateSearchIndexConfig.php --startOver
 php forceSearchIndex.php --forceUpdate

B.  Build a copy of the index, reindex to it, and then force a full reindex from source documents.  Uses
more disk space but search should be up the entire time:
 php updateSearchIndexConfig.php --reindexAndRemoveOk --indexIdentifier now
 php forceSearchIndex.php --forceUpdate

C.  Perform an in place index update with a brief close then a full reindex from source documents.  This
only brings down search for a few seconds _but_ it might be incompatible with some updates.  Safest to
never use it unless you know for sure:
 php updateSearchIndexConfig.php --closeOk
 php forceSearchIndex.php --forceUpdate

If you must do (2) really have only one option:
A.  Build of a copy of the index and reindex to it:
 php updateSearchIndexConfig.php --reindexAndRemoveOk --indexIdentifier now
 php forceSearchIndex.php --from <time when you started updateSearchIndexConfig.php in YYYY-mm-ddTHH:mm:ssZ> --deletes
 php forceSearchIndex.php --from <time when you started updateSearchIndexConfig.php in YYYY-mm-ddTHH:mm:ssZ>
or for the Bash inclined:
 TZ=UTC export REINDEX_START=$(date +%Y-%m-%dT%H:%m:%SZ)
 php updateSearchIndexConfig.php --reindexAndRemoveOk --indexIdentifier now
 php forceSearchIndex.php --from $REINDEX_START --deletes
 php forceSearchIndex.php --from $REINDEX_START

If you must do (3) you have two options:
A.  Same as (2.A)
B.  Perform an in place index update with a brief close. This only brings down search for a few seconds
_but_ it might be incompatible with some updates.  Safest to never use it unless you know for sure:
 php updateSearchIndexConfig.php --closeOk

4 is easy!

The safest thing if you don't know what is required for your update is to execute (1.B).


Production suggestions
----------------------

Elasticsearch

All the general rules for making Elasticsearch production ready apply here.  So you don't have to go
round them up below is a list.  Some of these steps are obvious, others will take some research.
1.  Have >= 3 nodes.
2.  Configure Elasticsearch for memlock.
3.  Change each node's elasticsearch.yml file in a few ways.
3a.  Change node name to the real host name.
3b.  Turn off autocreation and some other scary stuff by adding this (tested with 0.90.4):
 ################################### Actions #################################
 ## Modulo some small changes to comments this section comes directly from the
 ## wonderful Elasticsearch mailing list, specifically Dan Everton.
 ##
 # Require explicit index creation.  ES never autocreates the indexes the way we
 # like them.
 ##
 action.auto_create_index: false

 ##
 # Protect against accidental close/delete operations on all indices. You can
 # still close/delete individual indices.
 ##
 action.disable_close_all_indices: true
 action.disable_delete_all_indices: true

 ##
 # Disable ability to shutdown nodes via REST API.
 ##
 action.disable_shutdown: true


CirrusSearch

You should change the number of replicas to at least one, two if you want protection from crashes
during upgrades.  More replicas will help distribute queries across more nodes.  Having more
replicas than nodes doesn't make sense and Elasticsearch will stay "yellow" if you do that.
Anyway, you should add this to LocalSettings.php to change it:
 $wgCirrusSearchReplicaCount = array( 'content' => 2, 'general' => 2 );


Testing
-------
See tests/browser/README


Job Queue
---------
Cirrus makes heavy use of the job queue.  You can run it without any job queue customization but
if you switch the job queue to Redis with checkDelay enabled then Cirrus's results will be more
correct.  The reason for this is that this configuration allows Cirrus to delay link counts
until Elasticsearch has appropriately refreshed.  This is an example of configuring it:
 $wgRedisPassword = '<password goes here>';
 $wgJobTypeConf['default'] = array(
	'class' => 'JobQueueRedis',
	'order' => 'fifo',
	'redisServer' => 'localhost',
	'checkDelay' => true,
	'redisConfig' => array(
		'password' => $wgRedisPassword,
	),
 );
 $wgJobQueueAggregator = array(
	'class'       => 'JobQueueAggregatorRedis',
	'redisServer' => 'localhost',
	'redisConfig' => array(
		'password' => $wgRedisPassword,
	),
 );


Hooks
-----
CirrusSearch provides hooks that other extensions can make use of to extend the core schema and
modify documents.

There are currently two phases to building cirrus documents: the parse phase and the links phase.
The parse phase then the links phase is run when the article's rendered text would change (actual
article change and template change).  Only the links phase is run when an article is newly links
or unlinked.

Note that this whole thing is a somewhat experimental feature at this point and the API hasn't
really been settled.

'CirrusSearchAnalysisConfig': Allows to hook into the configuration for analysis
 &config - multi-dimensional configuration array for analysis of various languages and fields
 $builder - instance of MappingConfigBuilder, for easier use of utility methods to build fields

'CirrusSearchMappingConfig': Allows configuration of the mapping of fields
 &config - multi-dimensional configuration array that contains page metadata

Licensing information
---------------------
CirrusSearch makes use of the Elastica library to connect to elasticsearch <http://elastica.io/>.
It is Apache licensed and you can read the license Elastica/LICENSE.txt.

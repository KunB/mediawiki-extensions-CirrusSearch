MediaWiki extension: CirrusSearch
---------------------------------


Installation
------------
Fetch this plugin plugin into your extensions directory.
Make sure you have the curl php library installed (sudo apt-get install php5-curl in Debian.)
You also need to install the Elastica MediaWiki extension.
Add this to LocalSettings.php:
 require_once( "$IP/extensions/Elastica/Elastica.php" );
 require_once( "$IP/extensions/CirrusSearch/CirrusSearch.php" );
 $wgDisableSearchUpdate = true;

Configure your search servers in LocalSettings.php:
 $wgCirrusSearchServers = array( 'elasticsearch0', 'elasticsearch1', 'elasticsearch2', 'elasticsearch3' );
There are other $wgCirrusSearch variables that you might want to change from their defaults.
In production setups, you will very likely want to increase the number of replicas using
$wgCirrusSearchContentReplicaCount.
If you want to change them then set their new values with $wgCirrusSearchServers in LocalSettings.php.

Now run this script to generate your elasticsearch index:
 php ./maintenance/updateSearchIndexConfig.php

Now remove $wgDisableSearchUpdate = true from LocalSettings.php.  Updates should start heading to elasticsearch.

Next bootstrap the search index by running:
 php ./maintenance/forceSearchIndex.php --forceUpdate --skipLinks --indexOnSkip
 php ./maintenance/forceSearchIndex.php --forceUpdate --skipParse
Note that this can take some time.  For large wikis read Bootstrapping large wikis below.

Once that is complete add this to LocalSettings.php to funnel queries to ElasticSearch:
 $wgSearchType = 'CirrusSearch';


Bootstrapping large wikis
-------------------------
The --batch-size parameter controls the number documents read from MySQL and indexed into elasticsearch at
one time.  It defaults to 50 but you should feel free to play with it.  Too low causes transport overhead (sql
and Elasticsearch) but too high can make the process feel sluggish and can get you into trouble with the
OOM Killer.

forceSeachIndex.php accepts the --fromId and --toId parameters which can be used to split up the
work of bootstrapping the wiki into multiple processes.  Since most of the load on search indexing is on the
indexing script in the php process you should be able to break the process into multiple chunks and farm
them out to multiple php processes/machines.  The --buildChunks argument of forceSearchIndex.php will cause
the script to build invocations of itself that you can splay out to different processes.  For example:
 rm -rf /tmp/index_log
 mkdir /tmp/index_log
 php forceSearchIndex.php --buildChunks 10 --forceUpdate --skipLinks --indexOnSkip --batch-size 100 |
   xargs -I{} -t -P4 sh -c 'php {} > /tmp/index_log/$$.log'

forceSearchIndex.php also accepts --queue which can be used to enqueue all of its work onto the job queue
rather than running in process.  They will then be processed by whatever job queue consumers you have set
up.  Keep in mind that each one of these jobs can be slow and that adding all of them can cause a significant
backlog on the queue.  This is really only a good idea if you have a very robust job queue infrastructure.
This mechanism is more susceptible to the OOM Killer so you should make sure to keep the batch sizes low.
The default is fine.

Handling elasticsearch outages
------------------------------
If for some reason in process updates to elasticsearch begin failing you can immediately
set "$wgDisableSearchUpdate = true;" in your LocalSettings.php file to
stop trying to update elasticsearch.  Once you figure out what is wrong with elasticsearch you
should turn those updates back on and then run the following:
php ./maintenance/forceSearchIndex.php --from <whenever the outage started in ISO 8601 format> --deletes
php ./maintenance/forceSearchIndex.php --from <whenever the outage started in ISO 8601 format>

The first command picks up all the deletes that occurred during the outage and
should complete quite quickly.  The second command picks up all the updates
that occurred during the outage and might take significantly longer.


PoolCounter
-----------
CirrusSearch can leverage the PoolCounter extension to limit the number of concurrent requests to
elasticsearch.  You can do this by installing the PoolCounter extension and then configuring it in
LocalSettings.php like so:
 require_once( "$IP/extensions/PoolCounter/PoolCounterClient.php");
 $wgPoolCounterConf = array(
	'CirrusSearch-Update' => array(  // Configuration for all index updates not run by maintenance scripts
		'class' => 'PoolCounter_Client',
		'timeout' => 120,
		'workers' => 20,
		'maxqueue' => 200,
	),
	'CirrusSearch-Search' => array(  // Configuration for all searches
		'class' => 'PoolCounter_Client',
		'timeout' => 30,
		'workers' => 50,
		'maxqueue' => 10,
	)
 );


Upgrading
---------
When you upgrade there three possible cases for maintaining the index:
1.  You must update the index configuration and reindex from source documents.
2.  You must update the index configuration and reindex from already indexed documents.
3.  You must update the index configuration but no reindex is required.
4.  No changes are required.

If you must do (1) you have three options:
A.  Blow away the search index and rebuild it from scratch.  Marginally faster and uses less disk space on
in elasticsearch but empties the index entirely and rebuilds it so search will be down for a while:
 php updateSearchIndexConfig.php --startOver
 php forceSearchIndex.php --forceUpdate

B.  Build a copy of the index, reindex to it, and then force a full reindex from source documents.  Uses
more disk space but search should be up the entire time:
 php updateSearchIndexConfig.php --reindexAndRemoveOk --indexIdentifier now
 php forceSearchIndex.php --forceUpdate

C.  Perform an in place index update with a brief close then a full reindex from source documents.  This
only brings down search for a few seconds _but_ it might be incompatible with some updates.  Safest to
never use it unless you know for sure:
 php updateSearchIndexConfig.php --closeOk
 php forceSearchIndex.php --forceUpdate

If you must do (2) really have only one option:
A.  Build of a copy of the index and reindex to it:
 php updateSearchIndexConfig.php --reindexAndRemoveOk --indexIdentifier now
 php forceSearchIndex.php --from <time when you started updateSearchIndexConfig.php in YYYY-mm-ddTHH:mm:ssZ> --deletes
 php forceSearchIndex.php --from <time when you started updateSearchIndexConfig.php in YYYY-mm-ddTHH:mm:ssZ>
or for the Bash inclined:
 TZ=UTC export REINDEX_START=$(date +%Y-%m-%dT%H:%m:%SZ)
 php updateSearchIndexConfig.php --reindexAndRemoveOk --indexIdentifier now
 php forceSearchIndex.php --from $REINDEX_START --deletes
 php forceSearchIndex.php --from $REINDEX_START

If you must do (3) you have two options:
A.  Same as (2.A)
B.  Perform an in place index update with a brief close. This only brings down search for a few seconds
_but_ it might be incompatible with some updates.  Safest to never use it unless you know for sure:
 php updateSearchIndexConfig.php --closeOk

4 is easy!

The safest thing if you don't know what is required for your update is to execute (1.B).


Production suggestions
----------------------

Elasticsearch

All the general rules for making Elasticsearch production ready apply here.  So you don't have to go
round them up below is a list.  Some of these steps are obvious, others will take some research.
1.  Have >= 3 nodes.
2.  Configure Elasticsearch for memlock.
3.  Change each node's elasticsearch.yml file in a few ways.
3a.  Change node name to the real host name.
3b.  Turn off autocreation and some other scary stuff by adding this (tested with 0.90.4):
 ################################### Actions #################################
 ## Modulo some small changes to comments this section comes directly from the
 ## wonderful Elasticsearch mailing list, specifically Dan Everton.
 ##
 # Require explicit index creation.  ES never autocreates the indexes the way we
 # like them.
 ##
 action.auto_create_index: false

 ##
 # Protect against accidental close/delete operations on all indices. You can
 # still close/delete individual indices.
 ##
 action.disable_close_all_indices: true
 action.disable_delete_all_indices: true

 ##
 # Disable ability to shutdown nodes via REST API.
 ##
 action.disable_shutdown: true


CirrusSearch

You should change the number of replicas to at least one, two if you want protection from crashes
during upgrades.  More replicas will help distribute queries across more nodes.  Having more
replicas than nodes doesn't make sense and Elasticsearch will stay "yellow" if you do that.
Anyway, you should add this to LocalSettings.php to change it:
 $wgCirrusSearchContentReplicaCount = array( 'content' => 2, 'general' => 2 );


Licensing information
---------------------
CirrusSearch makes use of the Elastica library to connect to elasticsearch <http://elastica.io/>.
It is Apache licensed and you can read the license Elastica/LICENSE.txt.
